{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tarupathak30/rag-astronomy-chatbot-/blob/main/bot_astro_exoplanets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# create folder\n",
    "folder_name = \"exoplanet_data\"\n",
    "os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(folder_name, \"planets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/content/exoplanets.json\"  # your existing file\n",
    "destination = os.path.join(folder_name, \"/content/exoplanet_data/exoplanets.json\")\n",
    "\n",
    "shutil.move(source, destination)\n",
    "\n",
    "print(\"File moved:\", destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, math, torch, faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID       = \"google/gemma-2-2b-it\"\n",
    "EMBED_MODEL    = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TOP_K          = 5\n",
    "DEBUG          = True   # set True to inspect intermediate steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()   # It will ask you to paste your token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Gemma...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None\n",
    ")\n",
    "print(\"Loaded Gemma on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL, device=DEVICE)\n",
    "embed_dim = embed_model.get_sentence_embedding_dimension()\n",
    "print(\"Embedding dimension =\", embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- UTILITIES ----------\n",
    "def planet_to_text(obj):\n",
    "    \"\"\"Flatten planet JSON object into readable text.\"\"\"\n",
    "    parts = [obj.get(\"planet_name\", \"Unnamed Planet\")]\n",
    "    for k, v in obj.items():\n",
    "        if isinstance(v, dict):\n",
    "            parts.append(f\"{k}: \" + \", \".join(f\"{ik}={iv}\" for ik, iv in v.items()))\n",
    "        elif k != \"planet_name\":\n",
    "            parts.append(f\"{k}: {v}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def chunk_text(text, max_len=300):\n",
    "    words = text.split()\n",
    "    chunks, cur = [], []\n",
    "\n",
    "    for w in words:\n",
    "        cur.append(w)\n",
    "        if len(cur) >= max_len:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur = []\n",
    "\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- INDEX BUILDING ----------\n",
    "def build_index(planets):\n",
    "    embeddings = []\n",
    "    meta_objects = []\n",
    "\n",
    "    for planet in planets:\n",
    "        text = planet_to_text(planet)\n",
    "        chunks = chunk_text(text)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            emb = embed_model.encode([chunk], convert_to_numpy=True)[0].astype(\"float32\")\n",
    "            embeddings.append(emb)\n",
    "            meta_objects.append(planet)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    print(\"Total chunks:\", len(embeddings))\n",
    "    print(\"Embedding shape:\", embeddings.shape)\n",
    "\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    meta = {\"objects\": meta_objects}\n",
    "    print(\"FAISS index built:\", index.ntotal)\n",
    "    return index, meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- RETRIEVAL ----------\n",
    "def retrieve_objects(query, index, meta, top_k=5):\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb)\n",
    "\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "\n",
    "    results = []\n",
    "    seen = set()\n",
    "\n",
    "    for idx in I[0]:\n",
    "        planet = meta[\"objects\"][idx]\n",
    "        pid = id(planet)\n",
    "        if pid not in seen:\n",
    "            seen.add(pid)\n",
    "            results.append(planet)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- FACT CHECK LOGIC ----------\n",
    "def fact_check(question, candidates):\n",
    "    q = question.lower()\n",
    "\n",
    "    # Longest orbital period\n",
    "    if \"longest orbital period\" in q:\n",
    "        return max(candidates, key=lambda c: c[\"planet_profile\"].get(\"orbital_period_days\", -1))\n",
    "\n",
    "    # Shortest orbital period\n",
    "    if \"shortest orbital period\" in q:\n",
    "        return min(candidates, key=lambda c: c[\"planet_profile\"].get(\"orbital_period_days\", 1e18))\n",
    "\n",
    "    # Largest radius\n",
    "    if \"largest radius\" in q or \"biggest planet\" in q:\n",
    "        return max(candidates, key=lambda c: c[\"planet_profile\"].get(\"radius_earth_radii\", -1))\n",
    "\n",
    "    # Smallest radius\n",
    "    if \"smallest radius\" in q:\n",
    "        return min(candidates, key=lambda c: c[\"planet_profile\"].get(\"radius_earth_radii\", 1e18))\n",
    "\n",
    "    # fallback = semantic reasoning\n",
    "    scores = [\n",
    "        embed_model.encode([question + \" \" + c[\"planet_name\"]])[0].sum()\n",
    "        for c in candidates\n",
    "    ]\n",
    "    return candidates[int(np.argmax(scores))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- MAIN QA FUNCTION ----------\n",
    "def answer_query(question, index, meta, top_k=5):\n",
    "    print(\"\\nüîç Query:\", question)\n",
    "\n",
    "    cands = retrieve_objects(question, index, meta, top_k)\n",
    "    print(\"\\nRetrieved:\", [c[\"planet_name\"] for c in cands])\n",
    "\n",
    "    if not cands:\n",
    "        return \"No planets found.\"\n",
    "\n",
    "    selected = fact_check(question, cands)\n",
    "\n",
    "    return {\n",
    "        \"answer\": f\"The planet most aligned with your query is **{selected['planet_name']}**.\",\n",
    "        \"planet\": selected\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- LOAD ----------\n",
    "def load_planets(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# ---------- RUN ----------\n",
    "planets = load_planets(\"/content/exoplanet_data/exoplanets.json\")\n",
    "index, meta = build_index(planets)\n",
    "\n",
    "out = answer_query(\"Which exoplanets were discovered in 2015?\", index, meta)\n",
    "\n",
    "print(\"\\n=== FINAL OUTPUT ===\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_PROMPT = \"\"\"\n",
    "You are a query-to-filter translator for an astronomy database.\n",
    "Given a natural language question about exoplanets, return ONLY a JSON object\n",
    "describing the structured filters. Do NOT add commentary.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Q: \"Which planets were discovered in 2015?\"\n",
    "A: {{\"filter\": {{\"year\": 2015}}}}\n",
    "\n",
    "Q: \"Planets with eccentricity greater than 0.5\"\n",
    "A: {{\"filter\": {{\"planet_profile.eccentricity\": {{\">$\": 0.5}}}}}}\n",
    "\n",
    "Q: \"Show me planets orbiting K-type stars\"\n",
    "A: {{\"filter\": {{\"host_star.spectral_type\": \"K\"}}}}\n",
    "\n",
    "If no structured filter exists, return:\n",
    "{{\"filter\": null}}\n",
    "\n",
    "Now convert the following question:\n",
    "\n",
    "Q: {query}\n",
    "A:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_json_str(text):\n",
    "    # Extract the first JSON object found in the string\n",
    "    match = re.search(r'{.*?}', text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return '{}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_to_filter(query):\n",
    "    prompt = FILTER_PROMPT.format(query=query)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    device = next(llm.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = llm.generate(**inputs, max_new_tokens=100)\n",
    "    out_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Raw LLM output:\", out_str)  # Debug to see raw string\n",
    "\n",
    "    json_str = clean_json_str(out_str)\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"JSON decode error:\", e)\n",
    "        return {\"filter\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "OPS = {\n",
    "    \">\":  operator.gt,\n",
    "    \"<\":  operator.lt,\n",
    "    \">=\": operator.ge,\n",
    "    \"<=\": operator.le,\n",
    "    \"==\": operator.eq,\n",
    "}\n",
    "\n",
    "def get_nested(obj, path):\n",
    "    \"\"\"Access nested fields like host_star.temperature_k\"\"\"\n",
    "    parts = path.split(\".\")\n",
    "    for p in parts:\n",
    "        obj = obj.get(p)\n",
    "        if obj is None:\n",
    "            return None\n",
    "    return obj\n",
    "\n",
    "\n",
    "def apply_filter(planets, filt):\n",
    "    if filt is None:\n",
    "        return planets  # no filtering, return all\n",
    "\n",
    "    def get_nested(d, keys):\n",
    "        for key in keys:\n",
    "            if d is None or key not in d:\n",
    "                return None\n",
    "            d = d[key]\n",
    "        return d\n",
    "\n",
    "    filtered = []\n",
    "    for planet in planets:\n",
    "        match = True\n",
    "        for k, v in filt.items():\n",
    "            keys = k.split(\".\")\n",
    "            val = get_nested(planet, keys)\n",
    "            if isinstance(v, dict):\n",
    "                # Handle operators like {\">$\": 0.5}, etc.\n",
    "                for op, comp_val in v.items():\n",
    "                    if op == \">$\" and not (val is not None and val > comp_val):\n",
    "                        match = False\n",
    "                    # Add other ops as needed\n",
    "            else:\n",
    "                if val != v:\n",
    "                    match = False\n",
    "            if not match:\n",
    "                break\n",
    "        if match:\n",
    "            filtered.append(planet)\n",
    "    return filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_filter(question, index, meta, top_k=10):\n",
    "    # Step 1: LLM ‚Üí filter\n",
    "    filt = llm_to_filter(question)[\"filter\"]\n",
    "\n",
    "    # Step 2: Apply filter if exists\n",
    "    filtered_planets = apply_filter(meta[\"objects\"], filt) if filt else meta[\"objects\"]\n",
    "\n",
    "    if not filtered_planets:\n",
    "        return []\n",
    "\n",
    "    # Step 3: Semantic narrowing\n",
    "    # turn into mini-index\n",
    "    tmp_texts = [planet_to_text(p) for p in filtered_planets]\n",
    "    tmp_embs = embed_model.encode(tmp_texts)\n",
    "    tmp_embs = tmp_embs.astype(\"float32\")\n",
    "\n",
    "    q_emb = embed_model.encode([question]).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    faiss.normalize_L2(tmp_embs)\n",
    "\n",
    "    D, I = faiss.IndexFlatIP(tmp_embs.shape[1]).search(tmp_embs, 1)\n",
    "\n",
    "    # rank filtered planets by their cosine similarity\n",
    "    ranked = sorted(\n",
    "        zip(filtered_planets, D.squeeze().tolist()),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return [p for p, score in ranked[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_v2(question, index, meta):\n",
    "    print(\"\\nüîç Query:\", question)\n",
    "\n",
    "    cands = retrieve_with_filter(question, index, meta)\n",
    "\n",
    "    if not cands:\n",
    "        return \"No matching planets found.\"\n",
    "\n",
    "    best = cands[0]\n",
    "\n",
    "    return {\n",
    "        \"answer\": f\"Best match: **{best['planet_name']}**\",\n",
    "        \"planet\": best,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = answer_query_v2(\"Which exoplanets were discovered in 2015?\", index, meta)\n",
    "\n",
    "print(\"\\n=== FINAL OUTPUT ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat as nbf\n",
    "\n",
    "fname = \"exoplanets.ipynb\"\n",
    "out = \"bot_exoplanets_.ipynb\"\n",
    "\n",
    "nb = nbf.read(fname, as_version=4)\n",
    "\n",
    "# remove metadata that GitHub hates\n",
    "for cell in nb[\"cells\"]:\n",
    "    if \"metadata\" in cell:\n",
    "        for key in [\"id\", \"colab\", \"outputId\", \"executionInfo\"]:\n",
    "            cell[\"metadata\"].pop(key, None)\n",
    "\n",
    "# remove notebook-level widget metadata\n",
    "for key in [\"colab\", \"widgets\"]:\n",
    "    nb[\"metadata\"].pop(key, None)\n",
    "\n",
    "nbf.write(nb, out)\n",
    "print(\"üî• Cleaned! Upload bot_astro_exoplanets_GITHUB.ipynb to GitHub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
