{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarupathak30/rag-astronomy-chatbot-/blob/main/exoplanets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG_5CDfP79yv"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers faiss-cpu transformers langchain_community\n",
        "!pip install -q astroquery\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3CuFTaIqQIC",
        "outputId": "6c0f5815-ed71-42b3-844b-85b089f04afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "page = wikipedia.page(\"Brown dwarf\")\n"
      ],
      "metadata": {
        "id": "uKWW5kEPt_ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(page.content[:500])\n"
      ],
      "metadata": {
        "id": "b7abCCBauAuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "keywords = [\n",
        "    \"Brown dwarf\",\n",
        "    \"Gas giant\",\n",
        "    \"Planet\",\n",
        "    \"Star\",\n",
        "    \"Main sequence\",\n",
        "    \"Exoplanet\",\n",
        "    \"Radial velocity\",\n",
        "    \"Transit method\",\n",
        "    \"Direct imaging\",\n",
        "    \"Neutron star\",\n",
        "    \"White dwarf\",\n",
        "    \"Galaxy\",\n",
        "    \"Dark matter\",\n",
        "    \"Dark energy\",\n",
        "    \"Supernova\",\n",
        "    \"Black hole\",\n",
        "    \"Protoplanetary disk\",\n",
        "    \"Astrobiology\"\n",
        "]\n",
        "\n",
        "concept_texts = []\n",
        "\n",
        "for topic in keywords:\n",
        "    try:\n",
        "        page = wikipedia.page(topic)\n",
        "        if(topic == \"Brown dwarf\"):\n",
        "          print(page)\n",
        "        concept_texts.append((topic, page.content))\n",
        "    except:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "-zB_pBIUqIeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "mawA6sU3PEUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ðŸ”­ 2. IMPORTS\n",
        "# ============================\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "\n",
        "# ============================\n",
        "# ðŸ”­ 3. DOWNLOAD EXOPLANET DATA\n",
        "# ============================\n",
        "exo = NasaExoplanetArchive.query_criteria(table=\"pscomppars\")\n",
        "exoplanets = exo.to_pandas()\n",
        "exoplanets.fillna(\"\", inplace=True)\n",
        "exoplanets.to_csv(\"exoplanets.csv\", index=False)\n",
        "\n",
        "print(\"Downloaded exoplanet rows:\", len(exoplanets))\n",
        "\n",
        "# ============================\n",
        "# ðŸ”­ 4. BUILD DOCUMENTS\n",
        "# ============================\n"
      ],
      "metadata": {
        "id": "62W6Y4t58E_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 3ï¸âƒ£ Build system-level docs\n",
        "# -------------------------------\n",
        "def build_docs(exo):\n",
        "    docs = []\n",
        "    grouped = exo.group_by(\"hostname\")\n",
        "    for group in grouped.groups:\n",
        "        star = group['hostname'][0]\n",
        "        context_text = \"\"\n",
        "        for row in group:\n",
        "            context_text += (\n",
        "                f\"planet_name: {row['pl_name']}\\n\"\n",
        "                f\"orbital_period_days: {row['pl_orbper']}\\n\"\n",
        "                f\"mass_earth: {row['pl_bmasse']}\\n\"\n",
        "                f\"radius_earth: {row['pl_rade']}\\n\\n\"\n",
        "            )\n",
        "        docs.append(Document(page_content=context_text, metadata={\"host_star\": star}))\n",
        "    print(f\"Built {len(docs)} system-level documents\")\n",
        "    return docs"
      ],
      "metadata": {
        "id": "wCNuhKmXzYgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = build_docs(exo)"
      ],
      "metadata": {
        "id": "0FlM8PFZ6PFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------\n",
        "# 4ï¸âƒ£ Load embeddings + FAISS\n",
        "# -------------------------------\n",
        "def build_vector_index(docs):\n",
        "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    texts = [d.page_content for d in docs]\n",
        "    metadatas = [d.metadata for d in docs]\n",
        "    index = FAISS.from_texts(texts=texts, embedding=embedder, metadatas=metadatas)\n",
        "    index.save_local(\"faiss_index\")\n",
        "    pickle.dump(docs, open(\"docs.pkl\", \"wb\"))\n",
        "    print(\"âœ… FAISS index and docs saved successfully\")\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "lNJnWGOgqkRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = build_vector_index(docs)"
      ],
      "metadata": {
        "id": "n7tetgYa6Ujb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 6ï¸âƒ£ Load LLM + tokenizer\n",
        "# -------------------------------\n",
        "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "device = next(llm.parameters()).device\n",
        "print(\"LLM loaded on:\", device)"
      ],
      "metadata": {
        "id": "gARXtBQJzzTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "reranker = CrossEncoder(\"BAAI/bge-reranker-base\")"
      ],
      "metadata": {
        "id": "98UcAe4ZR6xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_docs(query, docs):\n",
        "    texts = [d.page_content for d in docs]\n",
        "    pairs = [(query, t) for t in texts]\n",
        "    scores = reranker.predict(pairs)\n",
        "    best_idx = int(scores.argmax())\n",
        "    return docs[best_idx]  # âœ… return Document object\n"
      ],
      "metadata": {
        "id": "UiWzKpIc-KrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"exoplanets.csv\")\n"
      ],
      "metadata": {
        "id": "pdn4TNdMB86G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = df.columns.tolist()\n",
        "len(cols)"
      ],
      "metadata": {
        "id": "r2AFybwQnCjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['pl_name'].str.lower()==\"wasp-18 b\", ['pl_name','pl_orbper','pl_orbpererr1','pl_orbpererr2']]\n"
      ],
      "metadata": {
        "id": "VtPAUcooFhLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['pl_name'].str.lower()==\"epler-1604 b\", ['pl_rade', 'pl_name','pl_orbper','pl_orbpererr1','pl_orbpererr2', 'pl_bmasse']]\n"
      ],
      "metadata": {
        "id": "sRKazbhphrew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# FIXED: METADATA + HELPERS + PIPELINE\n",
        "# ================================================\n",
        "import pandas as pd\n",
        "import re\n",
        "import difflib\n",
        "import torch\n",
        "from typing import Optional\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# -------------------------------\n",
        "# Basic helpers\n",
        "# -------------------------------\n",
        "def extract_number(value):\n",
        "    \"\"\"Extract numeric value from strings, return float or None.\"\"\"\n",
        "    if pd.isna(value):\n",
        "        return None\n",
        "    if isinstance(value, (int, float)):\n",
        "        return float(value)\n",
        "    s = str(value)\n",
        "    # Remove commas used as thousands separators\n",
        "    s = s.replace(\",\", \"\")\n",
        "    match = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
        "    return float(match[0]) if match else None\n",
        "\n",
        "\n",
        "def tokenize(col_name):\n",
        "    \"\"\"Split words for fuzzy matching.\"\"\"\n",
        "    return re.findall(r'\\w+', str(col_name).lower())\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Robust column mapper\n",
        "# -------------------------------\n",
        "def map_query_to_column(query, columns):\n",
        "    \"\"\"\n",
        "    Preferred deterministic mapping for common questions (period, mass, radius).\n",
        "    Falls back to fuzzy matching only if none of the keywords are present.\n",
        "    \"\"\"\n",
        "    q = query.lower()\n",
        "    if any(k in q for k in [\"orbital\", \"orbite\", \"orb\", \"period\"]):\n",
        "        if \"pl_orbper\" in columns:\n",
        "            return \"pl_orbper\"\n",
        "    if any(k in q for k in [\"mass\", \"heavy\", \"heaviest\", \"most massive\"]):\n",
        "        if \"pl_bmasse\" in columns:\n",
        "            return \"pl_bmasse\"\n",
        "        if \"pl_massj\" in columns:\n",
        "            return \"pl_massj\"\n",
        "    if any(k in q for k in [\"radius\", \"size\", \"largest radius\"]):\n",
        "        if \"pl_rade\" in columns:\n",
        "            return \"pl_rade\"\n",
        "\n",
        "    # fallback fuzzy matching (lower priority)\n",
        "    query_toks = tokenize(q)\n",
        "    best_col, best_score = None, 0\n",
        "    for col in columns:\n",
        "        col_toks = tokenize(col)\n",
        "        for qt in query_toks:\n",
        "            for ct in col_toks:\n",
        "                score = difflib.SequenceMatcher(None, qt, ct).ratio()\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_col = col\n",
        "    return best_col\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Normalize units for orbital period\n",
        "# -------------------------------\n",
        "def normalize_orbital_column_to_days(series):\n",
        "    \"\"\"\n",
        "    Attempt to normalize pl_orbper to days.\n",
        "    Heuristic:\n",
        "      - many values < 1 -> likely in years -> convert (years * 365.25)\n",
        "      - otherwise assume already in days\n",
        "    Returns a numeric Series (days) with NaNs preserved.\n",
        "    \"\"\"\n",
        "    nums = series.map(extract_number)\n",
        "    # how many numeric values < 1?\n",
        "    valid = nums.dropna()\n",
        "    if valid.empty:\n",
        "        return nums  # nothing to do\n",
        "    frac_lt1 = (valid < 1).sum() / len(valid)\n",
        "    if frac_lt1 > 0.5:\n",
        "        # more than half < 1 -> assume years -> convert to days\n",
        "        return nums * 365.25\n",
        "    return nums\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Build text context for LLM\n",
        "# -------------------------------\n",
        "def build_context(row):\n",
        "    \"\"\"Return a readable text block from a Series / dict-like row.\"\"\"\n",
        "    ctx = []\n",
        "    if hasattr(row, \"items\"):\n",
        "        iterator = row.items()\n",
        "    else:\n",
        "        iterator = row._asdict().items()\n",
        "    for col, val in iterator:\n",
        "        if pd.notna(val):\n",
        "            ctx.append(f\"{col}: {val}\")\n",
        "    return \"\\n\".join(ctx)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Get top row robustly (max/min)\n",
        "# -------------------------------\n",
        "def get_top_row(df, column, query=None, prefer_max=True):\n",
        "    \"\"\"\n",
        "    Finds the top row (max or min) for column.\n",
        "    - Ensures numeric conversion via extract_number\n",
        "    - For pl_orbper, normalizes to days first\n",
        "    - Returns full row (Series) or None\n",
        "    \"\"\"\n",
        "    df2 = df.copy()\n",
        "\n",
        "    # special normalization for orbital period column\n",
        "    if column == \"pl_orbper\":\n",
        "        df2[\"_norm_orbper_days\"] = normalize_orbital_column_to_days(df2[column])\n",
        "        val_col = \"_norm_orbper_days\"\n",
        "    else:\n",
        "        df2[column] = df2[column].map(extract_number)\n",
        "        val_col = column\n",
        "\n",
        "    df2 = df2.dropna(subset=[val_col])\n",
        "    if df2.empty:\n",
        "        return None\n",
        "\n",
        "    # interpret query words for min/max\n",
        "    if query:\n",
        "        q = query.lower()\n",
        "        if any(w in q for w in [\"shortest\", \"smallest\", \"least\"]):\n",
        "            prefer_max = False\n",
        "\n",
        "    if prefer_max:\n",
        "        idx = df2[val_col].idxmax()\n",
        "    else:\n",
        "        idx = df2[val_col].idxmin()\n",
        "\n",
        "    return df2.loc[idx]\n",
        "\n",
        "\n",
        "# ================================================\n",
        "# 2) ANSWER ENGINE (STRUCTURED + DOC + LLM) - FIXED\n",
        "# ================================================\n",
        "def answer_query(query, df, docs, tok, llm, device):\n",
        "    \"\"\"\n",
        "    Fixed answer_query:\n",
        "      - robust column detection\n",
        "      - numeric normalization for orbital period (days)\n",
        "      - only retrieve docs for the selected planet\n",
        "      - safe tokenization & device movement\n",
        "      - low creativity generation (temperature=0.0, no sampling)\n",
        "    \"\"\"\n",
        "    # 1) map query -> column\n",
        "    col = map_query_to_column(query, df.columns)\n",
        "    if not col:\n",
        "        return \"No matching column found.\"\n",
        "\n",
        "    # 2) get the winner row (use query to decide min/max)\n",
        "    row = get_top_row(df, col, query=query, prefer_max=True)\n",
        "    if row is None:\n",
        "        return \"No valid numeric data found for this column.\"\n",
        "\n",
        "    # 3) build structured context from that single row\n",
        "    structured = build_context(row)\n",
        "\n",
        "    # 4) retrieve docs only for this planet (docs are Document objects)\n",
        "    planet_name = str(row.get(\"pl_name\", \"\")).lower()\n",
        "    related_docs = []\n",
        "    for d in docs:\n",
        "        # d may be a Document or a plain string â€” handle both\n",
        "        text = d.page_content if isinstance(d, Document) else str(d)\n",
        "        if planet_name and planet_name in text.lower():\n",
        "            related_docs.append(text)\n",
        "    top_doc = related_docs[0] if related_docs else \"No retrieved documents available.\"\n",
        "\n",
        "    # 5) build the prompt (short & strict to avoid hallucination)\n",
        "    prompt = (\n",
        "        \"You are an astronomy assistant. Use ONLY the data below. Do NOT invent or combine planets.\\n\\n\"\n",
        "        \"Structured data (single selected row):\\n\"\n",
        "        f\"{structured}\\n\\n\"\n",
        "        \"System-level data (about the same planet):\\n\"\n",
        "        f\"{top_doc}\\n\\n\"\n",
        "        f\"Question: {query}\\n\\n\"\n",
        "        \"Final Answer:\"\n",
        "    )\n",
        "\n",
        "    # 6) tokenize safely (tokenizer returns a dict of tensors)\n",
        "    inputs = tok(prompt, return_tensors=\"pt\")\n",
        "    # move tensors to device safely\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # 7) generate (low temperature, deterministic)\n",
        "    with torch.no_grad():\n",
        "        # prefer passing explicit input_ids and attention_mask if present\n",
        "        gen_kwargs = {\n",
        "            \"input_ids\": inputs.get(\"input_ids\"),\n",
        "            \"max_new_tokens\": 150,\n",
        "            \"temperature\": 0.0,\n",
        "            \"do_sample\": False\n",
        "        }\n",
        "        if \"attention_mask\" in inputs:\n",
        "            gen_kwargs[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
        "\n",
        "        outputs = llm.generate(**gen_kwargs)\n",
        "\n",
        "    # 8) decode and extract final answer\n",
        "    text = tok.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"Final Answer:\" in text:\n",
        "        text = text.split(\"Final Answer:\")[-1].strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "wHwcGoSBjBsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = answer_query(\n",
        "    \"Which exoplanet has the longest orbital period?\",\n",
        "    df,\n",
        "    docs,\n",
        "    tok,\n",
        "    llm,\n",
        "    device\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "iTqDMh03bszT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6lu7lWqWb9Gh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}