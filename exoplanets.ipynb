{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarupathak30/rag-astronomy-chatbot-/blob/main/exoplaents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG_5CDfP79yv",
        "outputId": "dbd526ad-3448-4c5a-89a3-1b97c111af0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q sentence-transformers faiss-cpu transformers langchain_community\n",
        "!pip install -q astroquery\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3CuFTaIqQIC",
        "outputId": "eb630737-2ea1-4cd6-d528-2a29da3cc51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=eb91f60412598eafa5f2cb495fd7f7a84fedde4609235820c2dbab06808e50f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "page = wikipedia.page(\"Brown dwarf\")\n"
      ],
      "metadata": {
        "id": "uKWW5kEPt_ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(page.content[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7abCCBauAuc",
        "outputId": "3cd29f6f-1dc6-40bf-a257-b0f00e07193a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brown dwarfs are substellar objects that have more mass than the biggest gas giant planets, but less than the least massive main-sequence stars.  Their mass is approximately 13 to 80 times that of Jupiter (MJ)â€”not big enough to sustain nuclear fusion of hydrogen into helium in their cores, but massive enough to emit some light and heat from the fusion of deuterium, 2H, an isotope of hydrogen with a neutron as well as a proton, that can undergo fusion at lower temperatures.  The most massive ones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "keywords = [\n",
        "    \"Brown dwarf\",\n",
        "    \"Gas giant\",\n",
        "    \"Planet\",\n",
        "    \"Star\",\n",
        "    \"Main sequence\",\n",
        "    \"Exoplanet\",\n",
        "    \"Radial velocity\",\n",
        "    \"Transit method\",\n",
        "    \"Direct imaging\",\n",
        "    \"Neutron star\",\n",
        "    \"White dwarf\",\n",
        "    \"Galaxy\",\n",
        "    \"Dark matter\",\n",
        "    \"Dark energy\",\n",
        "    \"Supernova\",\n",
        "    \"Black hole\",\n",
        "    \"Protoplanetary disk\",\n",
        "    \"Astrobiology\"\n",
        "]\n",
        "\n",
        "concept_texts = []\n",
        "\n",
        "for topic in keywords:\n",
        "    try:\n",
        "        page = wikipedia.page(topic)\n",
        "        if(topic == \"Brown dwarf\"):\n",
        "          print(page)\n",
        "        concept_texts.append((topic, page.content))\n",
        "    except:\n",
        "        pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zB_pBIUqIeJ",
        "outputId": "2f9b663e-42d9-416f-c983-0b86e127e02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<WikipediaPage 'Brown dwarf'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ğŸ”­ 2. IMPORTS\n",
        "# ============================\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "\n",
        "# ============================\n",
        "# ğŸ”­ 3. DOWNLOAD EXOPLANET DATA\n",
        "# ============================\n",
        "exo = NasaExoplanetArchive.query_criteria(table=\"pscomppars\")\n",
        "exoplanets = exo.to_pandas()\n",
        "exoplanets.fillna(\"\", inplace=True)\n",
        "exoplanets.to_csv(\"exoplanets.csv\", index=False)\n",
        "\n",
        "print(\"Downloaded exoplanet rows:\", len(exoplanets))\n",
        "\n",
        "# ============================\n",
        "# ğŸ”­ 4. BUILD DOCUMENTS\n",
        "# ============================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62W6Y4t58E_R",
        "outputId": "be2eaa3f-6c36-47a5-d1c9-291a830439b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3209938367.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  exoplanets.fillna(\"\", inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded exoplanet rows: 6045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = []\n",
        "\n",
        "# Group by 'hostname'\n",
        "grouped = exo.group_by(\"hostname\")\n",
        "\n",
        "for group in grouped.groups:  # each group is a QTable subset\n",
        "    star = group['hostname'][0]  # all rows in this group have the same host\n",
        "    context_text = \"\"\n",
        "\n",
        "    for row in group:  # iterate rows in the QTable group\n",
        "        context_text += (\n",
        "            f\"planet_name: {row['pl_name']}\\n\"\n",
        "            f\"orbital_period_days: {row['pl_orbper']}\\n\"\n",
        "            f\"mass_earth: {row['pl_bmasse']}\\n\"\n",
        "            f\"radius_earth: {row['pl_rade']}\\n\\n\"\n",
        "        )\n",
        "\n",
        "    docs.append(\n",
        "        Document(\n",
        "            page_content=context_text,\n",
        "            metadata={\"host_star\": star}\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(f\"Built {len(docs)} system-level documents\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbtfVyjBxqIP",
        "outputId": "8553ae6d-38fc-4160-955c-b8c172c17b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: column st_lum has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_lum has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_lumerr1 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_lumerr1 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_lumerr2 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_lumerr2 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_logg has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_logg has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_loggerr1 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_loggerr1 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_loggerr2 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_loggerr2 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_lum has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_lum has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_lumerr1 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_lumerr1 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_lumerr2 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_lumerr2 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_logg has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_logg has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_loggerr1 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_loggerr1 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n",
            "WARNING: column st_loggerr2 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\") [astropy.table.table]\n",
            "WARNING:astroquery:column st_loggerr2 has a unit but is kept as a MaskedColumn as an attempt to convert it to Quantity failed with:\n",
            "UnitTypeError(\"MaskedQuantity instances require normal units, not <class 'astropy.units.function.logarithmic.DexUnit'> instances.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1ï¸âƒ£ Load reranker\n",
        "# ----------------------------\n",
        "from sentence_transformers import CrossEncoder\n",
        "reranker = CrossEncoder(\"BAAI/bge-reranker-base\")  # CPU or GPU\n",
        "\n",
        "# ----------------------------\n",
        "# 2ï¸âƒ£ Load LLM + tokenizer\n",
        "# ----------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "device = next(llm.parameters()).device\n",
        "print(\"LLM and reranker loaded on:\", device)\n"
      ],
      "metadata": {
        "id": "cpMQqsvBAKYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import pickle\n",
        "\n",
        "# Use LangChain wrapper\n",
        "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "texts = [d.page_content for d in docs]\n",
        "metadatas = [d.metadata for d in docs]\n",
        "\n",
        "index = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embedder,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "index.save_local(\"faiss_index\")\n",
        "\n",
        "# Save docs separately\n",
        "with open(\"docs.pkl\", \"wb\") as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "print(\"âœ… FAISS + docs.pkl saved successfully.\")\n"
      ],
      "metadata": {
        "id": "33RLWKpr8xQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concept_docs = []\n",
        "\n",
        "for title, content in concept_texts:\n",
        "    concept_docs.append(Document(\n",
        "        page_content=content,\n",
        "        metadata={\"source\": title}\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "lNJnWGOgqkRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs = docs + concept_docs\n",
        "texts = [d.page_content for d in all_docs]\n",
        "metadatas = [d.metadata for d in all_docs]\n",
        "\n",
        "index = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embedder,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "index.save_local(\"faiss_index\")\n",
        "pickle.dump(all_docs, open(\"docs.pkl\",\"wb\"))\n"
      ],
      "metadata": {
        "id": "qt2H-ZIoqlg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ğŸ”­ 7. RERANK FUNCTION\n",
        "# ============================\n",
        "def rerank_docs(query, docs):\n",
        "    \"\"\"\n",
        "    Rerank candidate docs using CrossEncoder and return the top doc content.\n",
        "    \"\"\"\n",
        "    texts = [d.page_content for d in docs]\n",
        "    pairs = [(query, t) for t in texts]\n",
        "    scores = reranker.predict(pairs)  # returns numpy array\n",
        "    best_idx = int(scores.argmax())\n",
        "    return docs[best_idx].page_content"
      ],
      "metadata": {
        "id": "UiWzKpIc-KrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_query(query):\n",
        "  # adding diversity in the document selection\n",
        "    # 1. semantic search\n",
        "    hits = index.max_marginal_relevance_search(query, k=10, fetch_k=30)\n",
        "    if not hits:\n",
        "        return \"No data available.\"\n",
        "\n",
        "    # 2. rerank\n",
        "    context = rerank_docs(query, hits)\n",
        "\n",
        "    # 3. build strict prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are AstroBuddy, a friendly astronomy nerd who loves exoplanets, stars, and exoplanet systems.\n",
        "You explain things clearly, logically, and with a touch of humor or nerdiness when appropriate.\n",
        "ALWAYS follow these rules:\n",
        "1. You MUST answer using ONLY the text in <context>.\n",
        "2. Do NOT add, infer, or guess anything.\n",
        "3. If the answer is NOT directly stated in <context>, reply EXACTLY with:\n",
        "   \"I cannot answer using only the provided context.\"\n",
        "4. When answering, you can phrase things in a friendly or nerdy tone, but you cannot hallucinate facts or numbers.\n",
        "5. Numbers and units in the <context> are factual and must be used exactly as written.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "Question: {query}\n",
        "Answer clearly and concisely using ONLY the provided context.\n",
        "\"\"\"\n",
        "\n",
        "    # 4. llm generate\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = llm.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.05,\n",
        "            pad_token_id=tok.eos_token_id\n",
        "        )\n",
        "    answer = tok.decode(out[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "vkzPcouFsUEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"Which exoplanet has the longest orbital period?\"\n",
        "\n",
        "print(answer_query(query))\n"
      ],
      "metadata": {
        "id": "ZFbc3vTerUux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSX-K38xr9Qd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
