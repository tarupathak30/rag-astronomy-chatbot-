{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tarupathak30/rag-astronomy-chatbot-/blob/main/bot_astro_exoplanets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# create folder\n",
    "folder_name = \"exoplanet_data\"\n",
    "os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(folder_name, \"planets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/content/exoplanets.json\"  # your existing file\n",
    "destination = os.path.join(folder_name, \"/content/exoplanet_data/exoplanets.json\")\n",
    "\n",
    "shutil.move(source, destination)\n",
    "\n",
    "print(\"File moved:\", destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, math, torch, faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID       = \"google/gemma-2-2b-it\"\n",
    "EMBED_MODEL    = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TOP_K          = 5\n",
    "DEBUG          = True   # set True to inspect intermediate steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()   # It will ask you to paste your token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Gemma...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None\n",
    ")\n",
    "print(\"Loaded Gemma on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL, device=DEVICE)\n",
    "embed_dim = embed_model.get_sentence_embedding_dimension()\n",
    "print(\"Embedding dimension =\", embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- UTILITIES ----------\n",
    "def planet_to_text(obj):\n",
    "    name = obj.get(\"planet_name\", \"Unnamed Planet\")\n",
    "\n",
    "    profile = obj.get(\"planet_profile\", {})\n",
    "    star = obj.get(\"host_star\", {})\n",
    "    disc = obj.get(\"discovery\", {})\n",
    "    env = obj.get(\"environment\", {})\n",
    "\n",
    "    parts = [\n",
    "        f\"The planet {name} has a radius of {profile.get('radius_earth_radii')} Earth radii \"\n",
    "        f\"and a mass of {profile.get('mass_earth_masses')} Earth masses.\",\n",
    "        f\"It orbits its star every {profile.get('orbital_period_days')} days \"\n",
    "        f\"at a distance of {profile.get('semi_major_axis_au')} AU.\",\n",
    "        f\"The host star {star.get('name')} is a {star.get('spectral_type')} type star \"\n",
    "        f\"with a temperature of {star.get('temperature_k')} K.\",\n",
    "        f\"This planet was discovered in {disc.get('year')} \"\n",
    "        f\"using the {disc.get('method')} method at {disc.get('facility')}.\",\n",
    "        f\"The equilibrium temperature of the planet is {env.get('equilibrium_temperature_k')} K \"\n",
    "        f\"and it lies at a distance of {env.get('distance_pc')} parsecs from Earth.\"\n",
    "    ]\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "\n",
    "def chunk_text(text, max_len=400):\n",
    "    words = text.split()\n",
    "    chunks, cur = [], []\n",
    "\n",
    "    for w in words:\n",
    "        cur.append(w)\n",
    "        if len(cur) >= max_len:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur = []\n",
    "\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_chunks(data_list):\n",
    "    for obj in data_list[:3]:  # preview first 3 planets\n",
    "        text = planet_to_text(obj)\n",
    "        chunks = chunk_text(text, max_len=500)\n",
    "\n",
    "        print(f\"\\n======== {obj.get('planet_name')} ========\\n\")\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"\\n--- Chunk {i} ---\\n{chunk}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/content/exoplanet_data/exoplanets.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "preview_chunks(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- INDEX BUILDING ----------\n",
    "def build_index(planets):\n",
    "    embeddings = []\n",
    "    meta_objects = []\n",
    "\n",
    "    for planet in planets:\n",
    "        text = planet_to_text(planet)\n",
    "        chunks = chunk_text(text)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            emb = embed_model.encode([chunk], convert_to_numpy=True)[0].astype(\"float32\")\n",
    "            embeddings.append(emb)\n",
    "            meta_objects.append(planet)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    print(\"Total chunks:\", len(embeddings))\n",
    "    print(\"Embedding shape:\", embeddings.shape)\n",
    "\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    meta = {\"objects\": meta_objects}\n",
    "    print(\"FAISS index built:\", index.ntotal)\n",
    "    return index, meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- RETRIEVAL ----------\n",
    "def retrieve_objects(query, index, meta, top_k=5):\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb)\n",
    "\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "\n",
    "    results = []\n",
    "    seen = set()\n",
    "\n",
    "    for idx in I[0]:\n",
    "        planet = meta[\"objects\"][idx]\n",
    "        pid = id(planet)\n",
    "        if pid not in seen:\n",
    "            seen.add(pid)\n",
    "            results.append(planet)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_year(q):\n",
    "    match = re.search(r\"\\b(19|20)\\d{2}\\b\", q)\n",
    "    return int(match.group()) if match else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- FACT CHECK LOGIC ----------\n",
    "def fact_check(question, candidates):\n",
    "    q = question.lower()\n",
    "\n",
    "    # Longest orbital period\n",
    "    if \"longest orbital period\" in q:\n",
    "        return max(candidates, key=lambda c: c[\"planet_profile\"].get(\"orbital_period_days\", -1))\n",
    "\n",
    "    # Shortest orbital period\n",
    "    if \"shortest orbital period\" in q:\n",
    "        return min(candidates, key=lambda c: c[\"planet_profile\"].get(\"orbital_period_days\", 1e18))\n",
    "\n",
    "    # Largest radius\n",
    "    if \"largest radius\" in q or \"biggest planet\" in q:\n",
    "        return max(candidates, key=lambda c: c[\"planet_profile\"].get(\"radius_earth_radii\", -1))\n",
    "\n",
    "    # Smallest radius\n",
    "    if \"smallest radius\" in q:\n",
    "        return min(candidates, key=lambda c: c[\"planet_profile\"].get(\"radius_earth_radii\", 1e18))\n",
    "\n",
    "    # fallback = semantic reasoning\n",
    "    scores = [\n",
    "        embed_model.encode([question + \" \" + c[\"planet_name\"]])[0].sum()\n",
    "        for c in candidates\n",
    "    ]\n",
    "    return candidates[int(np.argmax(scores))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(question, index, meta, top_k=5):\n",
    "    print(\"\\nðŸ” Query:\", question)\n",
    "    cands = retrieve_objects(question, index, meta, top_k)\n",
    "    print(\"\\nRetrieved:\", [c[\"planet_name\"] for c in cands])\n",
    "\n",
    "    for p in cands:\n",
    "      print(p[\"planet_name\"], \"-->\", p.get(\"discovery\", {}).get(\"year\"))\n",
    "\n",
    "\n",
    "    if not cands:\n",
    "        return \"No planets found.\"\n",
    "\n",
    "    # ---- Year filtering ----\n",
    "    year = extract_year(question.lower())\n",
    "    if year:\n",
    "        filtered = [c for c in cands\n",
    "                    if c.get(\"discovery\", {}).get(\"year\") == year]\n",
    "\n",
    "        if filtered:\n",
    "            cands = filtered\n",
    "            print(f\"\\n Filtered by discovery year = {year}\")\n",
    "            print(\"Remaining:\", [c[\"planet_name\"] for c in cands])\n",
    "        else:\n",
    "            print(f\"\\n No exact matches for discovery year {year}.\")\n",
    "            print(\"Falling back to semantic candidates.\")\n",
    "\n",
    "    # ---- Fact-check selection ----\n",
    "    selected = fact_check(question, cands)\n",
    "\n",
    "    return {\n",
    "        \"answer\": f\"The planet most aligned with your query is **{selected['planet_name']}**.\",\n",
    "        \"planet\": selected\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- LOAD ----------\n",
    "def load_planets(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# ---------- RUN ----------\n",
    "planets = load_planets(\"/content/exoplanet_data/exoplanets.json\")\n",
    "index, meta = build_index(planets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = answer_query(\"Which exoplanets were discovered in 2015?\", index, meta)\n",
    "\n",
    "print(\"\\n=== FINAL OUTPUT ===\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
